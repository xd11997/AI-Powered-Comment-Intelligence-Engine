# LLM Generation Evaluation Framework

## 1. Purpose

This evaluation framework assesses the quality of themes generated by the LLM before they are used as semantic guidance for downstream DBSCAN clustering.

The goal is NOT to judge philosophical correctness of themes, but to ensure that the generated output is:

- Grounded in the data
- Semantically coherent
- Distinct across themes
- Suitable for embedding-based clustering

---

## 2. Evaluation Context

Input:
- 50 user comments (BTS-related)

LLM Task:
- Extract 5 major audience interest themes

Output:
- A list of 5 theme labels

Downstream usage:
- Theme interpretation
- Semantic validation of clustering results
- Embedding space interpretability

---

## 3. Evaluation Criteria

Each dimension is scored from **0 to 4**.

### 3.1 Relevance (0–4)

Measures whether themes are grounded in the actual comments.

| Score | Description |
|-------|-------------|
| 4 | All themes clearly supported by comment evidence |
| 3 | Most themes supported, minor over-generalization |
| 2 | Several themes weakly supported |
| 1 | Major themes not grounded in data |
| 0 | Mostly hallucinated themes |

---

### 3.2 Coherence (0–4)

Measures internal semantic consistency within each theme.

| Score | Description |
|-------|-------------|
| 4 | Each theme internally consistent |
| 3 | Minor inconsistencies |
| 2 | Noticeable mixing of unrelated concepts |
| 1 | Themes are poorly defined |
| 0 | Themes are semantically incoherent |

---

### 3.3 Distinctiveness (0–4)

Measures whether themes are meaningfully distinguishable from one another.

| Score | Description |
|-------|-------------|
| 4 | Clearly distinct themes |
| 3 | Minor overlaps |
| 2 | Significant semantic overlap |
| 1 | Themes largely redundant |
| 0 | Duplicate themes |

---

### 3.4 Coverage (0–4)

Measures how well the themes collectively explain the dataset distribution.

| Score | Description |
|-------|-------------|
| 4 | >80% comments reasonably covered |
| 3 | 60–80% covered |
| 2 | 40–60% covered |
| 1 | <40% covered |
| 0 | Themes fail to represent dataset |

Note:
Highly imbalanced distributions (e.g., one dominant theme) are acceptable as long as themes reflect the actual data structure.

---

## 4. Scoring Interpretation

Total Score = Sum of four dimensions (Max = 16)

| Total Score | Interpretation |
|------------|---------------|
| 14–16 | High quality – Suitable for downstream clustering |
| 10–13 | Acceptable – Minor over-generalization |
| 6–9 | Risky – Requires prompt refinement |
| 0–5 | Unreliable – Not suitable for clustering |

---

## 5. Common Failure Modes

- Hallucinated themes not supported by comments
- Overly abstract themes (e.g., "Emotional resonance")
- Data-structure themes mistaken for content themes
- Redundant theme splitting
- Ignoring dominant semantic cluster

---

## 6. Evaluation Procedure

1. Translate non-English comments if necessary.
2. Manually estimate theme frequency.
3. Score each dimension independently.
4. Record results in `evaluation_results.json`.

---

## 7. Design Philosophy

This evaluation prioritizes engineering reliability over theoretical thematic purity.

The objective is to ensure stable semantic inputs for embedding and clustering pipelines rather than to construct an academically exhaustive thematic taxonomy.
