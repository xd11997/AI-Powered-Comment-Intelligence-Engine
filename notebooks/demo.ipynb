{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16d4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 把项目根目录加入 Python path\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfad7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"audience_interest_themes\": [\n",
      "    \"Desire for more detailed content\",\n",
      "    \"Interest in practical and relatable advice\",\n",
      "    \"Appreciation for clear structure and pacing\",\n",
      "    \"Request for additional content or follow-ups\",\n",
      "    \"Curiosity about the process and time involved\"\n",
      "  ],\n",
      "  \"positive_content_drivers\": [\n",
      "    \"Calming editing style\",\n",
      "    \"Clear structure\",\n",
      "    \"Motivational and relatable content\"\n",
      "  ],\n",
      "  \"recurring_pain_points\": [\n",
      "    \"Content feels unrealistic\",\n",
      "    \"Perceived repetitiveness\",\n",
      "    \"Concerns about sponsorship and expense\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from src.data_loader import load_comments\n",
    "from src.chunking import chunk_comments\n",
    "from src.llm_extraction import extract_insights_from_chunk\n",
    "\n",
    "comments = load_comments(\"../data/lifestyle_comments.csv\")\n",
    "chunks = chunk_comments(comments, chunk_size=50)\n",
    "\n",
    "result = extract_insights_from_chunk(chunks[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d90ab4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. total=10 failed=0. failed indices=[]\n"
     ]
    }
   ],
   "source": [
    "# Notebook cell: 解析 LLM 输出并构建 group_results（使用 src/parsing.robust_parse）\n",
    "# 先确保你已经在 src/parsing.py 中实现了 robust_parse(raw_output, raise_on_fail=False)\n",
    "# 并且 extract_insights_from_chunk(chunk) 函数可用（它会向 LLM 发请求并返回原始字符串）。\n",
    "\n",
    "from src.parsing import robust_parse\n",
    "\n",
    "group_results = []\n",
    "failed = []\n",
    "\n",
    "# 防护：确保 chunks 存在\n",
    "try:\n",
    "    iterator = enumerate(chunks)\n",
    "except NameError:\n",
    "    raise NameError(\"变量 `chunks` 未定义。请先生成 chunks（例如按视频或文本分片）。\")\n",
    "\n",
    "for i, chunk in iterator:\n",
    "    # 1) 调用你的提取函数（这会返回 LLM 的原始字符串输出）\n",
    "    raw_output = extract_insights_from_chunk(chunk)\n",
    "\n",
    "    # 2) 用稳健解析器解析（不会在解析失败时抛异常，除非你改参数）\n",
    "    parsed = robust_parse(raw_output, raise_on_fail=False)\n",
    "\n",
    "    # 3) 处理解析失败的情况：记录并以空结构占位，避免后续 pipeline 崩溃\n",
    "    if parsed is None:\n",
    "        print(f\"[Warning] chunk #{i} parse failed — saving preview to debug file.\")\n",
    "        failed.append(i)\n",
    "        with open(f\"debug_raw_chunk_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            # 保存原始输出，便于离线检查（供你或我分析）\n",
    "            f.write(str(raw_output))\n",
    "        group_results.append({\n",
    "            \"audience_interest_themes\": [],\n",
    "            \"positive_content_drivers\": [],\n",
    "            \"recurring_pain_points\": []\n",
    "        })\n",
    "    else:\n",
    "        # 解析成功 -> 追加到结果列表\n",
    "        group_results.append(parsed)\n",
    "\n",
    "print(f\"Done. total={len(chunks)} failed={len(failed)}. failed indices={failed}\")\n",
    "# 现在你可以调用 aggregate_insights_with_clustering(group_results, eps=0.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d4d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Desire for more detailed content', 10), ('Interest in practical and relatable content', 8), ('Requests for additional content or variations', 5), ('Appreciation for clear structure and pacing', 4), ('Concerns about cost and affordability', 4)]\n",
      "[('Clear structure', 10), ('Motivational and relatable content', 5), ('Authenticity of the content', 5)]\n",
      "[('Content feels too sponsored', 10), ('Lack of detailed explanations', 4), ('Lengthy intros', 4)]\n"
     ]
    }
   ],
   "source": [
    "from src.aggregation import aggregate_insights_with_clustering\n",
    "final = aggregate_insights_with_clustering(group_results, eps=0.35)\n",
    "print(final['top_audience_interest_themes'])\n",
    "print(final['top_positive_content_drivers'])\n",
    "print(final['top_recurring_pain_points'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb59ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
