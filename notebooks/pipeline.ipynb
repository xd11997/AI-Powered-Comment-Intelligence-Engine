{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16d4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 把项目根目录加入 Python path\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfad7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_loader import load_comments\n",
    "from src.chunking import chunk_comments\n",
    "from src.llm_extraction_v2 import extract_insights_from_chunk\n",
    "\n",
    "comments = load_comments(\"../data/bts_comments.csv\")\n",
    "chunks = chunk_comments(comments, chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d90ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # （串发）Notebook cell: 解析 LLM 输出并构建 group_results（使用 src/parsing.robust_parse）\n",
    "# # 先确保你已经在 src/parsing.py 中实现了 robust_parse(raw_output, raise_on_fail=False)\n",
    "# # 并且 extract_insights_from_chunk(chunk) 函数可用（它会向 LLM 发请求并返回原始字符串）。\n",
    "\n",
    "# from src.parsing import robust_parse\n",
    "\n",
    "# group_results = []\n",
    "# failed = []\n",
    "\n",
    "# # 防护：确保 chunks 存在\n",
    "# try:\n",
    "#     iterator = enumerate(chunks[:6])\n",
    "# except NameError:\n",
    "#     raise NameError(\"变量 `chunks` 未定义。请先生成 chunks（例如按视频或文本分片）。\")\n",
    "\n",
    "# for i, chunk in iterator:\n",
    "#     # 1) 调用你的提取函数（这会返回 LLM 的原始字符串输出）\n",
    "#     raw_output = extract_insights_from_chunk(chunk)\n",
    "\n",
    "#     # 2) 用稳健解析器解析（不会在解析失败时抛异常，除非你改参数）\n",
    "#     parsed = robust_parse(raw_output, raise_on_fail=False)\n",
    "\n",
    "#     # 3) 处理解析失败的情况：记录并以空结构占位，避免后续 pipeline 崩溃\n",
    "#     if parsed is None:\n",
    "#         print(f\"[Warning] chunk #{i} parse failed — saving preview to debug file.\")\n",
    "#         failed.append(i)\n",
    "#         with open(f\"debug_raw_chunk_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#             # 保存原始输出，便于离线检查（供你或我分析）\n",
    "#             f.write(str(raw_output))\n",
    "#         group_results.append({\n",
    "#             \"audience_interest_themes\": [],\n",
    "#             \"positive_content_drivers\": [],\n",
    "#             \"recurring_pain_points\": []\n",
    "#         })\n",
    "#     else:\n",
    "#         # 解析成功 -> 追加到结果列表\n",
    "#         group_results.append(parsed)\n",
    "\n",
    "# print(f\"Done. total={len(group_results)} failed={len(failed)}. failed indices={failed}\")\n",
    "# # 现在你可以调用 aggregate_insights_with_clustering(group_results, eps=0.35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73558cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. total=6 failed=0. failed indices=[]\n"
     ]
    }
   ],
   "source": [
    "#（并发）\n",
    "from src.parsing import robust_parse\n",
    "import concurrent.futures\n",
    "\n",
    "group_results = []\n",
    "failed = []\n",
    "\n",
    "def process_chunk(i_chunk):\n",
    "    i, chunk = i_chunk\n",
    "    raw_output = extract_insights_from_chunk(chunk)\n",
    "    parsed = robust_parse(raw_output, raise_on_fail=False)\n",
    "\n",
    "    if parsed is None:\n",
    "        with open(f\"debug_raw_chunk_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(raw_output))\n",
    "        return i, {\n",
    "            'top_content_elements'\n",
    "            'top_audience_insights': [],\n",
    "            'top_engagement_drivers': [],\n",
    "            'top_audience_pain_points': []\n",
    "        }, True\n",
    "    else:\n",
    "        return i, parsed, False\n",
    "\n",
    "\n",
    "selected_chunks = list(enumerate(chunks[:6]))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(process_chunk, item) for item in selected_chunks]\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        i, result, is_failed = future.result()\n",
    "        group_results.append(result)\n",
    "        if is_failed:\n",
    "            failed.append(i)\n",
    "\n",
    "print(f\"Done. total={len(group_results)} failed={len(failed)}. failed indices={failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d4d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"BTS's humor and interactions\", 6), (\"Jimin's interactions\", 6), (\"Jin's personality\", 4), ('BTS V', 3), (\"BTS's concert performances\", 2)]\n",
      "[('Individuals who identify as ARMY and engage with BTS content regularly', 5), ('Fans from diverse backgrounds (e.g., Australian, Bangladeshi, Russian)', 3), ('International audience', 2)]\n",
      "[('Relatability through shared experiences and emotions', 6), (\"Emotional connections to the members' personalities and interactions\", 3), ('Desire for community interaction and shared fandom experiences', 3)]\n",
      "[('Requests for more content or specific performances', 2), ('Frustration with the absence of certain members in videos', 2), ('Confusion about song meanings or contexts', 2)]\n"
     ]
    }
   ],
   "source": [
    "from src.aggregation import aggregate_insights_with_clustering\n",
    "final = aggregate_insights_with_clustering(group_results, eps=0.35)\n",
    "print(final['top_content_elements'])\n",
    "print(final['top_audience_insights'])\n",
    "print(final['top_engagement_drivers'])\n",
    "print(final['top_audience_pain_points'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d24ff52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stability check\n",
    "\n",
    "# group_results = []\n",
    "# failed = []\n",
    "\n",
    "# def process_chunk(i_chunk):\n",
    "#     i, chunk = i_chunk\n",
    "#     raw_output = extract_insights_from_chunk(chunk)\n",
    "#     parsed = robust_parse(raw_output, raise_on_fail=False)\n",
    "\n",
    "#     if parsed is None:\n",
    "#         with open(f\"debug_raw_chunk_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(str(raw_output))\n",
    "#         return i, {\n",
    "#             \"audience_interest_themes\": [],\n",
    "#             \"positive_content_drivers\": [],\n",
    "#             \"recurring_pain_points\": []\n",
    "#         }, True\n",
    "#     else:\n",
    "#         return i, parsed, False\n",
    "\n",
    "\n",
    "# selected_chunks = list(enumerate(chunks[6:12]))\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "#     futures = [executor.submit(process_chunk, item) for item in selected_chunks]\n",
    "\n",
    "#     for future in concurrent.futures.as_completed(futures):\n",
    "#         i, result, is_failed = future.result()\n",
    "#         group_results.append(result)\n",
    "#         if is_failed:\n",
    "#             failed.append(i)\n",
    "\n",
    "# print(f\"Done. total={len(group_results)} failed={len(failed)}. failed indices={failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aff8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = aggregate_insights_with_clustering(group_results, eps=0.35)\n",
    "# print(final['top_content_elements'])\n",
    "# print(final['top_audience_insights'])\n",
    "# print(final['top_engagement_drivers'])\n",
    "# print(final['top_audience_pain_points'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eb59ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
